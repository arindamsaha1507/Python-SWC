{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0c4c6e3",
   "metadata": {},
   "source": [
    "# Defensive Programming\n",
    "\n",
    "**Teaching:** 30 min  \n",
    "**Exercises:** 10 min\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Explain what an assertion is.\n",
    "- Add assertions that check the program's state is correct.\n",
    "- Correctly add precondition and postcondition assertions to functions.\n",
    "- Explain what test-driven development is, and use it when creating new functions.\n",
    "- Explain why variables should be initialized using actual data values rather than arbitrary constants.\n",
    "\n",
    "## Questions\n",
    "\n",
    "- How can I make my programs more reliable?\n",
    "\n",
    "---\n",
    "\n",
    "Our previous lessons have introduced the basic tools of programming: variables and lists, file I/O, loops, conditionals, and functions. What they *haven't* done is show us how to tell whether a program is getting the right answer, and how to tell if it's *still* getting the right answer as we make changes to it.\n",
    "\n",
    "To achieve that, we need to:\n",
    "- Write programs that check their own operation.\n",
    "- Write and run tests for widely-used functions.\n",
    "- Make sure we know what \"correct\" actually means.\n",
    "\n",
    "The good news is, doing these things will speed up our programming, not slow it down. As in real carpentry — the kind done with lumber — the time saved by measuring carefully before cutting a piece of wood is much greater than the time that measuring takes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ff723c",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Let's start by importing the libraries we'll need and loading our inflammation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5caf443",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797aa2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our inflammation data\n",
    "data = np.loadtxt('../data/inflammation-01.csv', delimiter=',')\n",
    "print(f\"Data shape: {data.shape}\")\n",
    "print(f\"Data type: {data.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7732bfd5",
   "metadata": {},
   "source": [
    "## Assertions\n",
    "\n",
    "The first step toward getting the right answers from our programs is to assume that mistakes *will* happen and to guard against them. This is called **defensive programming**, and the most common way to do it is to add **assertions** to our code so that it checks itself as it runs.\n",
    "\n",
    "An assertion is simply a statement that something must be true at a certain point in a program. When Python sees one, it evaluates the assertion's condition. If it's true, Python does nothing, but if it's false, Python halts the program immediately and prints the error message if one is provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4d8450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Check that all inflammation values are non-negative\n",
    "numbers = [1.5, 2.3, 0.7, -0.001, 4.4]\n",
    "total = 0.0\n",
    "for num in numbers:\n",
    "    assert num >= 0.0, 'Data should only contain non-negative values'\n",
    "    total += num\n",
    "print('total is:', total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf99ce31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's fix the data and try again\n",
    "numbers = [1.5, 2.3, 0.7, 0.001, 4.4]  # Fixed negative value\n",
    "total = 0.0\n",
    "for num in numbers:\n",
    "    assert num >= 0.0, 'Data should only contain non-negative values'\n",
    "    total += num\n",
    "print('total is:', total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96098180",
   "metadata": {},
   "source": [
    "### Types of Assertions\n",
    "\n",
    "Broadly speaking, assertions fall into three categories:\n",
    "\n",
    "- A **precondition** is something that must be true at the start of a function in order for it to work correctly.\n",
    "- A **postcondition** is something that the function guarantees is true when it finishes.\n",
    "- An **invariant** is something that is always true at a particular point inside a piece of code.\n",
    "\n",
    "Let's create a function to analyze our inflammation data with proper assertions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e85f2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_inflammation(data):\n",
    "    \"\"\"Analyze inflammation data and return statistics.\n",
    "    \n",
    "    Parameters:\n",
    "    data: 2D numpy array of inflammation measurements\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary containing mean, max, and min values\n",
    "    \"\"\"\n",
    "    # Preconditions\n",
    "    assert isinstance(data, np.ndarray), 'Input must be a numpy array'\n",
    "    assert data.ndim == 2, 'Data must be 2-dimensional'\n",
    "    assert data.size > 0, 'Data cannot be empty'\n",
    "    assert np.all(data >= 0), 'All inflammation values must be non-negative'\n",
    "    \n",
    "    # Perform calculations\n",
    "    mean_inflammation = np.mean(data)\n",
    "    max_inflammation = np.max(data)\n",
    "    min_inflammation = np.min(data)\n",
    "    \n",
    "    # Create result dictionary\n",
    "    result = {\n",
    "        'mean': mean_inflammation,\n",
    "        'max': max_inflammation,\n",
    "        'min': min_inflammation\n",
    "    }\n",
    "    \n",
    "    # Postconditions\n",
    "    assert min_inflammation <= mean_inflammation <= max_inflammation, \\\n",
    "        'Mean should be between min and max values'\n",
    "    assert len(result) == 3, 'Result should contain exactly 3 statistics'\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f8453a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test our function with valid data\n",
    "stats = analyze_inflammation(data)\n",
    "print(f\"Inflammation statistics: {stats}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d472c07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with invalid data (this should trigger an assertion)\n",
    "try:\n",
    "    invalid_data = np.array([1, 2, -1, 4])  # Contains negative value\n",
    "    stats = analyze_inflammation(invalid_data)\n",
    "except AssertionError as e:\n",
    "    print(f\"Caught assertion error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a34302",
   "metadata": {},
   "source": [
    "### Rectangle Normalization Example\n",
    "\n",
    "Let's look at a more complex example. Suppose we are representing rectangles using a tuple of four coordinates `(x0, y0, x1, y1)`, representing the lower left and upper right corners of the rectangle. We need to normalize the rectangle so that the lower left corner is at the origin and the longest side is 1.0 units long:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c118969b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_rectangle(rect):\n",
    "    \"\"\"Normalizes a rectangle so that it is at the origin and 1.0 units long on its longest axis.\n",
    "    Input should be of the format (x0, y0, x1, y1).\n",
    "    (x0, y0) and (x1, y1) define the lower left and upper right corners\n",
    "    of the rectangle, respectively.\"\"\"\n",
    "    \n",
    "    # Preconditions\n",
    "    assert len(rect) == 4, 'Rectangles must contain 4 coordinates'\n",
    "    x0, y0, x1, y1 = rect\n",
    "    assert x0 < x1, 'Invalid X coordinates'\n",
    "    assert y0 < y1, 'Invalid Y coordinates'\n",
    "\n",
    "    dx = x1 - x0\n",
    "    dy = y1 - y0\n",
    "    if dx > dy:\n",
    "        scaled = dy / dx  # Fixed: was dx / dy\n",
    "        upper_x, upper_y = 1.0, scaled\n",
    "    else:\n",
    "        scaled = dx / dy\n",
    "        upper_x, upper_y = scaled, 1.0\n",
    "\n",
    "    # Postconditions\n",
    "    assert 0 < upper_x <= 1.0, 'Calculated upper X coordinate invalid'\n",
    "    assert 0 < upper_y <= 1.0, 'Calculated upper Y coordinate invalid'\n",
    "\n",
    "    return (0, 0, upper_x, upper_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2e13e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the rectangle normalization\n",
    "print(\"Normalizing a tall rectangle:\")\n",
    "print(normalize_rectangle((0.0, 0.0, 1.0, 5.0)))\n",
    "\n",
    "print(\"\\nNormalizing a wide rectangle:\")\n",
    "print(normalize_rectangle((0.0, 0.0, 5.0, 1.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f775f8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with invalid input\n",
    "try:\n",
    "    print(normalize_rectangle((0.0, 1.0, 2.0)))  # missing the fourth coordinate\n",
    "except AssertionError as e:\n",
    "    print(f\"Caught assertion error: {e}\")\n",
    "\n",
    "try:\n",
    "    print(normalize_rectangle((4.0, 2.0, 1.0, 5.0)))  # X axis inverted\n",
    "except AssertionError as e:\n",
    "    print(f\"Caught assertion error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0485a7",
   "metadata": {},
   "source": [
    "## Test-Driven Development\n",
    "\n",
    "An assertion checks that something is true at a particular point in the program. The next step is to check the overall behavior of a piece of code, i.e., to make sure that it produces the right output when it's given a particular input.\n",
    "\n",
    "Most novice programmers would solve this problem like this:\n",
    "1. Write a function `range_overlap`.\n",
    "2. Call it interactively on two or three different inputs.\n",
    "3. If it produces the wrong answer, fix the function and re-run that test.\n",
    "\n",
    "There's a better way:\n",
    "1. Write a short function for each test.\n",
    "2. Write a `range_overlap` function that should pass those tests.\n",
    "3. If `range_overlap` produces any wrong answers, fix it and re-run the test functions.\n",
    "\n",
    "Writing the tests *before* writing the function they exercise is called **test-driven development** (TDD)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb534b1",
   "metadata": {},
   "source": [
    "### Example: Range Overlap Function\n",
    "\n",
    "Suppose we need to find where two or more time series overlap. The range of each time series is represented as a pair of numbers, which are the time the interval started and ended. The output is the largest range that they all include.\n",
    "\n",
    "Let's start by defining an empty function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0c2cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def range_overlap(ranges):\n",
    "    \"\"\"Return common overlap among a set of [left, right] ranges.\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b75589",
   "metadata": {},
   "source": [
    "Now let's write some tests *before* implementing the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef7de11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These should fail initially since our function doesn't do anything yet\n",
    "try:\n",
    "    assert range_overlap([(0.0, 1.0)]) == (0.0, 1.0)\n",
    "    print(\"Test 1 passed\")\n",
    "except AssertionError:\n",
    "    print(\"Test 1 failed (expected for empty function)\")\n",
    "\n",
    "try:\n",
    "    assert range_overlap([(2.0, 3.0), (2.0, 4.0)]) == (2.0, 3.0)\n",
    "    print(\"Test 2 passed\")\n",
    "except AssertionError:\n",
    "    print(\"Test 2 failed (expected for empty function)\")\n",
    "\n",
    "try:\n",
    "    assert range_overlap([(0.0, 1.0), (0.0, 2.0), (-1.0, 1.0)]) == (0.0, 1.0)\n",
    "    print(\"Test 3 passed\")\n",
    "except AssertionError:\n",
    "    print(\"Test 3 failed (expected for empty function)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb939408",
   "metadata": {},
   "source": [
    "We also need to decide what to do when ranges don't overlap. We decide that:\n",
    "1. Every overlap has to have non-zero width\n",
    "2. We will return the special value `None` when there's no overlap\n",
    "\n",
    "Let's add tests for edge cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977ff20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test edge cases\n",
    "try:\n",
    "    assert range_overlap([(0.0, 1.0), (5.0, 6.0)]) == None  # No overlap\n",
    "    print(\"Edge test 1 passed\")\n",
    "except AssertionError:\n",
    "    print(\"Edge test 1 failed (expected for empty function)\")\n",
    "\n",
    "try:\n",
    "    assert range_overlap([(0.0, 1.0), (1.0, 2.0)]) == None  # Touch at endpoints\n",
    "    print(\"Edge test 2 passed\")\n",
    "except AssertionError:\n",
    "    print(\"Edge test 2 failed (expected for empty function)\")\n",
    "\n",
    "try:\n",
    "    assert range_overlap([]) == None  # Empty input\n",
    "    print(\"Edge test 3 passed\")\n",
    "except AssertionError:\n",
    "    print(\"Edge test 3 failed (expected for empty function)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60042b2c",
   "metadata": {},
   "source": [
    "Now let's implement the function properly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031842f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def range_overlap(ranges):\n",
    "    \"\"\"Return common overlap among a set of [left, right] ranges.\"\"\"\n",
    "    if not ranges:  # Handle empty input\n",
    "        return None\n",
    "    \n",
    "    # Initialize from the first range\n",
    "    max_left = ranges[0][0]\n",
    "    min_right = ranges[0][1]\n",
    "    \n",
    "    for (left, right) in ranges:\n",
    "        max_left = max(max_left, left)\n",
    "        min_right = min(min_right, right)\n",
    "    \n",
    "    # Check if there's actually an overlap\n",
    "    if max_left >= min_right:\n",
    "        return None\n",
    "    \n",
    "    return (max_left, min_right)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f3f8f1",
   "metadata": {},
   "source": [
    "Let's create a comprehensive test function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6e53c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_range_overlap():\n",
    "    \"\"\"Test the range_overlap function with various inputs.\"\"\"\n",
    "    assert range_overlap([(0.0, 1.0), (5.0, 6.0)]) == None\n",
    "    assert range_overlap([(0.0, 1.0), (1.0, 2.0)]) == None\n",
    "    assert range_overlap([(0.0, 1.0)]) == (0.0, 1.0)\n",
    "    assert range_overlap([(2.0, 3.0), (2.0, 4.0)]) == (2.0, 3.0)\n",
    "    assert range_overlap([(0.0, 1.0), (0.0, 2.0), (-1.0, 1.0)]) == (0.0, 1.0)\n",
    "    assert range_overlap([]) == None\n",
    "    print(\"All tests passed!\")\n",
    "\n",
    "# Run all tests\n",
    "test_range_overlap()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071957fc",
   "metadata": {},
   "source": [
    "## Applying Defensive Programming to Inflammation Data\n",
    "\n",
    "Let's create a robust function to analyze inflammation data across multiple files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21834eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_inflammation_files(filenames):\n",
    "    \"\"\"Analyze inflammation data from multiple files with defensive programming.\"\"\"\n",
    "    # Preconditions\n",
    "    assert isinstance(filenames, list), 'Filenames must be provided as a list'\n",
    "    assert len(filenames) > 0, 'At least one filename must be provided'\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for filename in filenames:\n",
    "        assert isinstance(filename, str), f'Filename must be a string: {filename}'\n",
    "        \n",
    "        try:\n",
    "            data = np.loadtxt(filename, delimiter=',')\n",
    "        except IOError:\n",
    "            print(f\"Warning: Could not read file {filename}\")\n",
    "            continue\n",
    "        except ValueError:\n",
    "            print(f\"Warning: Invalid data format in file {filename}\")\n",
    "            continue\n",
    "        \n",
    "        # Validate data\n",
    "        assert data.ndim == 2, f'Data in {filename} must be 2-dimensional'\n",
    "        assert data.size > 0, f'Data in {filename} cannot be empty'\n",
    "        \n",
    "        # Check for reasonable inflammation values\n",
    "        if np.any(data < 0):\n",
    "            print(f\"Warning: Negative values found in {filename}\")\n",
    "        if np.any(data > 20):\n",
    "            print(f\"Warning: Unusually high inflammation values in {filename}\")\n",
    "        \n",
    "        # Calculate statistics\n",
    "        file_stats = {\n",
    "            'filename': filename,\n",
    "            'shape': data.shape,\n",
    "            'mean': np.mean(data),\n",
    "            'max': np.max(data),\n",
    "            'min': np.min(data),\n",
    "            'std': np.std(data)\n",
    "        }\n",
    "        \n",
    "        # Postcondition: verify statistics make sense\n",
    "        assert file_stats['min'] <= file_stats['mean'] <= file_stats['max'], \\\n",
    "            f'Invalid statistics for {filename}'\n",
    "        assert file_stats['std'] >= 0, f'Standard deviation cannot be negative for {filename}'\n",
    "        \n",
    "        results.append(file_stats)\n",
    "    \n",
    "    # Final postcondition\n",
    "    assert len(results) > 0, 'No valid files were processed'\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ce7269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with our inflammation data files\n",
    "inflammation_files = glob.glob('../data/inflammation-*.csv')[:3]  # First 3 files\n",
    "print(f\"Analyzing files: {inflammation_files}\")\n",
    "\n",
    "results = analyze_inflammation_files(inflammation_files)\n",
    "\n",
    "for result in results:\n",
    "    print(f\"\\nFile: {result['filename']}\")\n",
    "    print(f\"  Shape: {result['shape']}\")\n",
    "    print(f\"  Mean: {result['mean']:.2f}\")\n",
    "    print(f\"  Range: {result['min']:.2f} - {result['max']:.2f}\")\n",
    "    print(f\"  Std Dev: {result['std']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9ef224",
   "metadata": {},
   "source": [
    "## Exercise: Creating Defensive Functions\n",
    "\n",
    "Let's practice creating defensive functions for common data analysis tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed4b438",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_average(values):\n",
    "    \"\"\"Calculate average with defensive programming.\n",
    "    \n",
    "    Your task: Add appropriate preconditions and postconditions\n",
    "    \"\"\"\n",
    "    # TODO: Add preconditions here\n",
    "    \n",
    "    result = sum(values) / len(values)\n",
    "    \n",
    "    # TODO: Add postconditions here\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Test the function\n",
    "test_data = [1, 2, 3, 4, 5]\n",
    "print(f\"Average of {test_data}: {safe_average(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94049c3d",
   "metadata": {},
   "source": [
    "### Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b082dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_average(values):\n",
    "    \"\"\"Calculate average with defensive programming.\"\"\"\n",
    "    # Preconditions\n",
    "    assert len(values) > 0, 'Cannot calculate average of empty sequence'\n",
    "    assert all(isinstance(x, (int, float)) for x in values), 'All values must be numeric'\n",
    "    \n",
    "    result = sum(values) / len(values)\n",
    "    \n",
    "    # Postconditions\n",
    "    assert min(values) <= result <= max(values), \\\n",
    "        'Average should be between min and max values'\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Test the function\n",
    "test_data = [1, 2, 3, 4, 5]\n",
    "print(f\"Average of {test_data}: {safe_average(test_data)}\")\n",
    "\n",
    "# Test with invalid data\n",
    "try:\n",
    "    safe_average([])\n",
    "except AssertionError as e:\n",
    "    print(f\"Caught expected error: {e}\")\n",
    "\n",
    "try:\n",
    "    safe_average([1, 2, 'three'])\n",
    "except AssertionError as e:\n",
    "    print(f\"Caught expected error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfaaf5f",
   "metadata": {},
   "source": [
    "## Best Practices for Defensive Programming\n",
    "\n",
    "1. **Fail early, fail often**: The greater the distance between when and where an error occurs and when it's noticed, the harder the error will be to debug.\n",
    "\n",
    "2. **Turn bugs into assertions or tests**: Whenever you fix a bug, write an assertion that catches the mistake should you make it again.\n",
    "\n",
    "3. **Initialize from data**: Always initialize variables using actual data values rather than arbitrary constants.\n",
    "\n",
    "4. **Write tests first**: Test-driven development helps you think about what your function should actually do.\n",
    "\n",
    "5. **Document your assumptions**: Use assertions to make your assumptions about data explicit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f3e856",
   "metadata": {},
   "source": [
    "## Exercise: Defensive Data Quality Check\n",
    "\n",
    "Create a function that performs comprehensive quality checks on inflammation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc65949b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_data_quality(data, filename=\"unknown\"):\n",
    "    \"\"\"Perform comprehensive quality checks on inflammation data.\n",
    "    \n",
    "    Returns a dictionary with quality metrics and warnings.\n",
    "    \"\"\"\n",
    "    quality_report = {\n",
    "        'filename': filename,\n",
    "        'valid': True,\n",
    "        'warnings': [],\n",
    "        'errors': []\n",
    "    }\n",
    "    \n",
    "    # Add your quality checks here\n",
    "    # Check for:\n",
    "    # - Correct data type and shape\n",
    "    # - Missing values (NaN)\n",
    "    # - Negative values\n",
    "    # - Outliers (values > 20)\n",
    "    # - Suspicious patterns (all zeros, all same value)\n",
    "    \n",
    "    return quality_report\n",
    "\n",
    "# Test with our data\n",
    "quality = check_data_quality(data, \"inflammation-01.csv\")\n",
    "print(quality)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc14d05",
   "metadata": {},
   "source": [
    "### Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e017acda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_data_quality(data, filename=\"unknown\"):\n",
    "    \"\"\"Perform comprehensive quality checks on inflammation data.\"\"\"\n",
    "    quality_report = {\n",
    "        'filename': filename,\n",
    "        'valid': True,\n",
    "        'warnings': [],\n",
    "        'errors': []\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Check data type and basic structure\n",
    "        if not isinstance(data, np.ndarray):\n",
    "            quality_report['errors'].append('Data is not a numpy array')\n",
    "            quality_report['valid'] = False\n",
    "            return quality_report\n",
    "        \n",
    "        if data.ndim != 2:\n",
    "            quality_report['errors'].append(f'Data should be 2D, got {data.ndim}D')\n",
    "            quality_report['valid'] = False\n",
    "        \n",
    "        if data.size == 0:\n",
    "            quality_report['errors'].append('Data is empty')\n",
    "            quality_report['valid'] = False\n",
    "            return quality_report\n",
    "        \n",
    "        # Check for missing values\n",
    "        if np.any(np.isnan(data)):\n",
    "            nan_count = np.sum(np.isnan(data))\n",
    "            quality_report['warnings'].append(f'Found {nan_count} missing values (NaN)')\n",
    "        \n",
    "        # Check for negative values\n",
    "        if np.any(data < 0):\n",
    "            neg_count = np.sum(data < 0)\n",
    "            quality_report['warnings'].append(f'Found {neg_count} negative values')\n",
    "        \n",
    "        # Check for outliers\n",
    "        if np.any(data > 20):\n",
    "            outlier_count = np.sum(data > 20)\n",
    "            max_val = np.max(data)\n",
    "            quality_report['warnings'].append(f'Found {outlier_count} outliers (>20), max={max_val:.2f}')\n",
    "        \n",
    "        # Check for suspicious patterns\n",
    "        if np.all(data == 0):\n",
    "            quality_report['warnings'].append('All values are zero')\n",
    "        \n",
    "        unique_values = len(np.unique(data))\n",
    "        total_values = data.size\n",
    "        if unique_values == 1:\n",
    "            quality_report['warnings'].append('All values are identical')\n",
    "        elif unique_values < total_values * 0.1:  # Less than 10% unique values\n",
    "            quality_report['warnings'].append(f'Low diversity: only {unique_values} unique values')\n",
    "        \n",
    "        # Add summary statistics\n",
    "        quality_report['stats'] = {\n",
    "            'shape': data.shape,\n",
    "            'mean': float(np.mean(data)),\n",
    "            'std': float(np.std(data)),\n",
    "            'min': float(np.min(data)),\n",
    "            'max': float(np.max(data)),\n",
    "            'unique_values': unique_values\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        quality_report['errors'].append(f'Unexpected error: {str(e)}')\n",
    "        quality_report['valid'] = False\n",
    "    \n",
    "    return quality_report\n",
    "\n",
    "# Test with our data\n",
    "quality = check_data_quality(data, \"inflammation-01.csv\")\n",
    "print(f\"Data quality report for {quality['filename']}:\")\n",
    "print(f\"Valid: {quality['valid']}\")\n",
    "print(f\"Warnings: {len(quality['warnings'])}\")\n",
    "for warning in quality['warnings']:\n",
    "    print(f\"  - {warning}\")\n",
    "print(f\"Errors: {len(quality['errors'])}\")\n",
    "for error in quality['errors']:\n",
    "    print(f\"  - {error}\")\n",
    "if 'stats' in quality:\n",
    "    print(f\"Stats: {quality['stats']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb083daf",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lesson, we learned about defensive programming techniques:\n",
    "\n",
    "1. **Assertions** help catch errors early and document assumptions about our code\n",
    "2. **Preconditions** check that function inputs are valid\n",
    "3. **Postconditions** verify that function outputs are correct\n",
    "4. **Test-driven development** involves writing tests before implementing functions\n",
    "5. **Defensive programming** makes our code more robust and easier to debug\n",
    "\n",
    "### Key Points\n",
    "\n",
    "- Program defensively: assume that errors are going to arise, and write code to detect them when they do\n",
    "- Put assertions in programs to check their state as they run, and to help readers understand how those programs are supposed to work\n",
    "- Use preconditions to check that the inputs to a function are safe to use\n",
    "- Use postconditions to check that the output from a function is safe to use\n",
    "- Write tests before writing code in order to help determine exactly what that code is supposed to do\n",
    "\n",
    "Defensive programming takes a little extra time up front, but it saves much more time in the long run by catching errors early and making code more reliable and maintainable."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
