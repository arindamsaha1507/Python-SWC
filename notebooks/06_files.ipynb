{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74ab8ac3",
   "metadata": {},
   "source": [
    "# Episode 6: Analyzing Data from Multiple Files\n",
    "\n",
    "Real research projects involve analyzing data from multiple files. In this notebook, we'll learn to work with files, process multiple datasets, and build automated analysis workflows for inflammation data.\n",
    "\n",
    "## Learning Objectives\n",
    "- Read and write files in Python\n",
    "- Process multiple data files automatically\n",
    "- Use glob patterns to find files\n",
    "- Handle different file formats (CSV, text)\n",
    "- Build file processing workflows\n",
    "- Organize and save analysis results\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In our inflammation study, we have data from multiple patients stored in separate files. Instead of analyzing each file manually, we'll learn to automate the process and analyze all files systematically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c879fe8",
   "metadata": {},
   "source": [
    "## 1. Reading Files\n",
    "\n",
    "Let's start with basic file operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9e7eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic file reading\n",
    "import os\n",
    "\n",
    "# Create a sample data file for demonstration\n",
    "sample_data = \"\"\"# Inflammation data for Patient P001\n",
    "# Day, Inflammation Level\n",
    "1, 0.0\n",
    "2, 1.5\n",
    "3, 3.2\n",
    "4, 4.1\n",
    "5, 2.8\n",
    "6, 1.9\n",
    "7, 0.8\n",
    "8, 0.0\n",
    "\"\"\"\n",
    "\n",
    "# Write sample file\n",
    "with open('sample_patient.txt', 'w') as f:\n",
    "    f.write(sample_data)\n",
    "\n",
    "print(\"Sample file created: sample_patient.txt\")\n",
    "print(\"File exists:\", os.path.exists('sample_patient.txt'))\n",
    "print(\"File size:\", os.path.getsize('sample_patient.txt'), \"bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d48b111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the entire file\n",
    "with open('sample_patient.txt', 'r') as f:\n",
    "    content = f.read()\n",
    "    \n",
    "print(\"File contents:\")\n",
    "print(content)\n",
    "print(f\"Content type: {type(content)}\")\n",
    "print(f\"Content length: {len(content)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4406083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading line by line\n",
    "print(\"Reading line by line:\")\n",
    "with open('sample_patient.txt', 'r') as f:\n",
    "    for line_number, line in enumerate(f, 1):\n",
    "        print(f\"Line {line_number}: {line.strip()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b75e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading all lines into a list\n",
    "with open('sample_patient.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "print(f\"Read {len(lines)} lines:\")\n",
    "for i, line in enumerate(lines):\n",
    "    print(f\"  {i}: {repr(line)}\")\n",
    "\n",
    "# Clean up lines (remove newlines and whitespace)\n",
    "clean_lines = [line.strip() for line in lines]\n",
    "print(f\"\\nCleaned lines:\")\n",
    "for i, line in enumerate(clean_lines):\n",
    "    print(f\"  {i}: '{line}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc342ff9",
   "metadata": {},
   "source": [
    "## 2. Parsing Data Files\n",
    "\n",
    "Extract meaningful data from files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd257376",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_inflammation_file(filename):\n",
    "    \"\"\"Parse an inflammation data file and extract numeric data.\"\"\"\n",
    "    inflammation_data = []\n",
    "    metadata = {}\n",
    "    \n",
    "    with open(filename, 'r') as f:\n",
    "        for line_num, line in enumerate(f, 1):\n",
    "            line = line.strip()\n",
    "            \n",
    "            # Skip empty lines\n",
    "            if not line:\n",
    "                continue\n",
    "            \n",
    "            # Handle comment lines (metadata)\n",
    "            if line.startswith('#'):\n",
    "                if 'Patient' in line:\n",
    "                    # Extract patient ID from comment\n",
    "                    parts = line.split()\n",
    "                    for part in parts:\n",
    "                        if part.startswith('P'):\n",
    "                            metadata['patient_id'] = part\n",
    "                continue\n",
    "            \n",
    "            # Parse data lines\n",
    "            try:\n",
    "                if ',' in line:\n",
    "                    # CSV format: day, inflammation\n",
    "                    day_str, inflammation_str = line.split(',')\n",
    "                    day = int(day_str.strip())\n",
    "                    inflammation = float(inflammation_str.strip())\n",
    "                    inflammation_data.append((day, inflammation))\n",
    "                else:\n",
    "                    # Single value per line\n",
    "                    inflammation = float(line)\n",
    "                    inflammation_data.append(inflammation)\n",
    "            except ValueError as e:\n",
    "                print(f\"Warning: Could not parse line {line_num}: '{line}' ({e})\")\n",
    "                continue\n",
    "    \n",
    "    return inflammation_data, metadata\n",
    "\n",
    "# Test the parser\n",
    "data, meta = parse_inflammation_file('sample_patient.txt')\n",
    "print(f\"Parsed data: {len(data)} readings\")\n",
    "print(f\"Metadata: {meta}\")\n",
    "print(f\"First few readings: {data[:5]}\")\n",
    "\n",
    "# Extract just the inflammation values\n",
    "if data and isinstance(data[0], tuple):\n",
    "    # Data contains (day, inflammation) tuples\n",
    "    inflammation_values = [reading[1] for reading in data]\n",
    "else:\n",
    "    # Data contains just inflammation values\n",
    "    inflammation_values = data\n",
    "\n",
    "print(f\"Inflammation values: {inflammation_values}\")\n",
    "print(f\"Average inflammation: {sum(inflammation_values) / len(inflammation_values):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2228add",
   "metadata": {},
   "source": [
    "## 3. Working with Multiple Files\n",
    "\n",
    "Create and process multiple patient files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5ef938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multiple sample patient files\n",
    "import random\n",
    "random.seed(42)  # For reproducible results\n",
    "\n",
    "# Create a data directory\n",
    "os.makedirs('inflammation_data', exist_ok=True)\n",
    "\n",
    "def generate_inflammation_pattern(days=10, patient_id=\"P000\"):\n",
    "    \"\"\"Generate a realistic inflammation pattern.\"\"\"\n",
    "    inflammation_data = []\n",
    "    \n",
    "    for day in range(1, days + 1):\n",
    "        # Simulate inflammation pattern: starts low, peaks, then decreases\n",
    "        if day <= days // 3:\n",
    "            # Early phase: increasing\n",
    "            base_level = day * 1.5\n",
    "        elif day <= 2 * days // 3:\n",
    "            # Peak phase: high and variable\n",
    "            base_level = 4.0 + random.uniform(-1.0, 2.0)\n",
    "        else:\n",
    "            # Recovery phase: decreasing\n",
    "            base_level = max(0.5, 5.0 - (day - 2 * days // 3) * 0.8)\n",
    "        \n",
    "        # Add some random variation\n",
    "        inflammation = max(0.0, base_level + random.uniform(-0.5, 0.5))\n",
    "        inflammation_data.append(round(inflammation, 1))\n",
    "    \n",
    "    return inflammation_data\n",
    "\n",
    "# Generate files for multiple patients\n",
    "patients = ['P001', 'P002', 'P003', 'P004', 'P005']\n",
    "file_info = []\n",
    "\n",
    "for patient_id in patients:\n",
    "    # Generate data\n",
    "    days = random.randint(8, 12)\n",
    "    inflammation_data = generate_inflammation_pattern(days, patient_id)\n",
    "    \n",
    "    # Create filename\n",
    "    filename = f'inflammation_data/inflammation_{patient_id.lower()}.csv'\n",
    "    \n",
    "    # Write file\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(f\"# Inflammation study data\\n\")\n",
    "        f.write(f\"# Patient: {patient_id}\\n\")\n",
    "        f.write(f\"# Days monitored: {days}\\n\")\n",
    "        f.write(f\"# Format: Day, Inflammation Level\\n\")\n",
    "        f.write(\"Day,Inflammation\\n\")  # Header\n",
    "        \n",
    "        for day, inflammation in enumerate(inflammation_data, 1):\n",
    "            f.write(f\"{day},{inflammation}\\n\")\n",
    "    \n",
    "    file_info.append({\n",
    "        'patient_id': patient_id,\n",
    "        'filename': filename,\n",
    "        'days': days,\n",
    "        'average': sum(inflammation_data) / len(inflammation_data)\n",
    "    })\n",
    "    \n",
    "    print(f\"Created {filename} for {patient_id} ({days} days, avg: {sum(inflammation_data) / len(inflammation_data):.2f})\")\n",
    "\n",
    "print(f\"\\nCreated {len(file_info)} patient files in inflammation_data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ff0cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List files in the directory\n",
    "import os\n",
    "\n",
    "data_dir = 'inflammation_data'\n",
    "files = os.listdir(data_dir)\n",
    "\n",
    "print(f\"Files in {data_dir}:\")\n",
    "for filename in sorted(files):\n",
    "    filepath = os.path.join(data_dir, filename)\n",
    "    size = os.path.getsize(filepath)\n",
    "    print(f\"  {filename:<25} ({size} bytes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a57ff4e",
   "metadata": {},
   "source": [
    "## 4. Using Glob to Find Files\n",
    "\n",
    "Find files using patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2200001",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "# Find all CSV files\n",
    "csv_files = glob.glob('inflammation_data/*.csv')\n",
    "print(f\"Found {len(csv_files)} CSV files:\")\n",
    "for file in sorted(csv_files):\n",
    "    print(f\"  {file}\")\n",
    "\n",
    "# Find files with specific patterns\n",
    "p00_files = glob.glob('inflammation_data/*p00*.csv')\n",
    "print(f\"\\nFiles matching pattern '*p00*': {len(p00_files)}\")\n",
    "for file in sorted(p00_files):\n",
    "    print(f\"  {file}\")\n",
    "\n",
    "# More specific patterns\n",
    "all_inflammation_files = glob.glob('inflammation_data/inflammation_*.csv')\n",
    "print(f\"\\nInflammation files: {len(all_inflammation_files)}\")\n",
    "for file in sorted(all_inflammation_files):\n",
    "    print(f\"  {os.path.basename(file)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc07db2",
   "metadata": {},
   "source": [
    "## 5. Processing Multiple Files\n",
    "\n",
    "Automated analysis of all patient files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe522a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_csv_inflammation_file(filename):\n",
    "    \"\"\"Parse CSV inflammation file with header.\"\"\"\n",
    "    inflammation_data = []\n",
    "    metadata = {'filename': filename}\n",
    "    \n",
    "    with open(filename, 'r') as f:\n",
    "        header_found = False\n",
    "        \n",
    "        for line_num, line in enumerate(f, 1):\n",
    "            line = line.strip()\n",
    "            \n",
    "            if not line:\n",
    "                continue\n",
    "            \n",
    "            # Handle comment lines\n",
    "            if line.startswith('#'):\n",
    "                if 'Patient:' in line:\n",
    "                    metadata['patient_id'] = line.split(':')[1].strip()\n",
    "                elif 'Days monitored:' in line:\n",
    "                    metadata['days_monitored'] = int(line.split(':')[1].strip())\n",
    "                continue\n",
    "            \n",
    "            # Skip header row\n",
    "            if 'Day' in line and 'Inflammation' in line:\n",
    "                header_found = True\n",
    "                continue\n",
    "            \n",
    "            # Parse data\n",
    "            try:\n",
    "                day_str, inflammation_str = line.split(',')\n",
    "                day = int(day_str.strip())\n",
    "                inflammation = float(inflammation_str.strip())\n",
    "                inflammation_data.append(inflammation)\n",
    "            except ValueError as e:\n",
    "                print(f\"Warning in {filename} line {line_num}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    return inflammation_data, metadata\n",
    "\n",
    "def analyze_inflammation_data(data):\n",
    "    \"\"\"Calculate statistics for inflammation data.\"\"\"\n",
    "    if not data:\n",
    "        return None\n",
    "    \n",
    "    stats = {\n",
    "        'count': len(data),\n",
    "        'sum': sum(data),\n",
    "        'average': sum(data) / len(data),\n",
    "        'minimum': min(data),\n",
    "        'maximum': max(data),\n",
    "    }\n",
    "    \n",
    "    # Add standard deviation\n",
    "    if len(data) > 1:\n",
    "        mean = stats['average']\n",
    "        variance = sum((x - mean)**2 for x in data) / (len(data) - 1)\n",
    "        stats['std_dev'] = variance ** 0.5\n",
    "    else:\n",
    "        stats['std_dev'] = 0.0\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Process all inflammation files\n",
    "def process_all_inflammation_files(pattern='inflammation_data/*.csv'):\n",
    "    \"\"\"Process all inflammation files and return results.\"\"\"\n",
    "    files = glob.glob(pattern)\n",
    "    results = []\n",
    "    \n",
    "    print(f\"Processing {len(files)} files...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for filename in sorted(files):\n",
    "        try:\n",
    "            # Parse file\n",
    "            data, metadata = parse_csv_inflammation_file(filename)\n",
    "            \n",
    "            # Analyze data\n",
    "            stats = analyze_inflammation_data(data)\n",
    "            \n",
    "            if stats:\n",
    "                result = {\n",
    "                    'filename': os.path.basename(filename),\n",
    "                    'patient_id': metadata.get('patient_id', 'Unknown'),\n",
    "                    'data': data,\n",
    "                    'statistics': stats,\n",
    "                    'status': 'success'\n",
    "                }\n",
    "                \n",
    "                # Print summary\n",
    "                print(f\"âœ… {result['patient_id']:<6} | \"\n",
    "                      f\"Days: {stats['count']:2d} | \"\n",
    "                      f\"Avg: {stats['average']:5.2f} | \"\n",
    "                      f\"Range: {stats['minimum']:4.1f}-{stats['maximum']:4.1f} | \"\n",
    "                      f\"SD: {stats['std_dev']:4.2f}\")\n",
    "            else:\n",
    "                result = {\n",
    "                    'filename': os.path.basename(filename),\n",
    "                    'patient_id': metadata.get('patient_id', 'Unknown'),\n",
    "                    'status': 'no_data'\n",
    "                }\n",
    "                print(f\"âš ï¸  {result['patient_id']:<6} | No valid data found\")\n",
    "            \n",
    "            results.append(result)\n",
    "            \n",
    "        except Exception as e:\n",
    "            result = {\n",
    "                'filename': os.path.basename(filename),\n",
    "                'patient_id': 'Unknown',\n",
    "                'status': 'error',\n",
    "                'error': str(e)\n",
    "            }\n",
    "            results.append(result)\n",
    "            print(f\"âŒ {os.path.basename(filename):<20} | Error: {e}\")\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    return results\n",
    "\n",
    "# Process all files\n",
    "analysis_results = process_all_inflammation_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c83104",
   "metadata": {},
   "source": [
    "### Exercise 6.1\n",
    "Create functions to:\n",
    "1. Find the patient with the highest average inflammation\n",
    "2. Identify patients with inflammation above a threshold\n",
    "3. Calculate overall study statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5148790b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 6.1 - Your analysis functions\n",
    "def find_highest_inflammation_patient(results):\n",
    "    \"\"\"Find patient with highest average inflammation.\"\"\"\n",
    "    # Your implementation here\n",
    "    pass\n",
    "\n",
    "def find_patients_above_threshold(results, threshold=3.0):\n",
    "    \"\"\"Find patients with average inflammation above threshold.\"\"\"\n",
    "    # Your implementation here\n",
    "    pass\n",
    "\n",
    "def calculate_study_statistics(results):\n",
    "    \"\"\"Calculate overall study statistics.\"\"\"\n",
    "    # Your implementation here\n",
    "    pass\n",
    "\n",
    "# Test your functions\n",
    "# Add your test code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb75fdad",
   "metadata": {},
   "source": [
    "## 6. Writing Analysis Results\n",
    "\n",
    "Save analysis results to files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f263e714",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_summary_report(results, output_filename='analysis_summary.txt'):\n",
    "    \"\"\"Write a comprehensive summary report.\"\"\"\n",
    "    \n",
    "    successful_results = [r for r in results if r['status'] == 'success']\n",
    "    \n",
    "    if not successful_results:\n",
    "        print(\"No successful analyses to report\")\n",
    "        return\n",
    "    \n",
    "    with open(output_filename, 'w') as f:\n",
    "        # Header\n",
    "        f.write(\"INFLAMMATION STUDY ANALYSIS REPORT\\n\")\n",
    "        f.write(\"=\" * 50 + \"\\n\")\n",
    "        f.write(f\"Generated on: {__import__('datetime').datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(f\"Total patients analyzed: {len(successful_results)}\\n\\n\")\n",
    "        \n",
    "        # Individual patient summaries\n",
    "        f.write(\"INDIVIDUAL PATIENT SUMMARIES\\n\")\n",
    "        f.write(\"-\" * 30 + \"\\n\")\n",
    "        \n",
    "        for result in successful_results:\n",
    "            stats = result['statistics']\n",
    "            f.write(f\"\\nPatient: {result['patient_id']}\\n\")\n",
    "            f.write(f\"  File: {result['filename']}\\n\")\n",
    "            f.write(f\"  Days monitored: {stats['count']}\\n\")\n",
    "            f.write(f\"  Average inflammation: {stats['average']:.3f}\\n\")\n",
    "            f.write(f\"  Range: {stats['minimum']:.1f} - {stats['maximum']:.1f}\\n\")\n",
    "            f.write(f\"  Standard deviation: {stats['std_dev']:.3f}\\n\")\n",
    "            \n",
    "            # Classification\n",
    "            if stats['average'] > 4.0:\n",
    "                classification = \"High risk\"\n",
    "            elif stats['average'] > 2.5:\n",
    "                classification = \"Moderate risk\"\n",
    "            else:\n",
    "                classification = \"Low risk\"\n",
    "            f.write(f\"  Risk level: {classification}\\n\")\n",
    "        \n",
    "        # Overall statistics\n",
    "        all_averages = [r['statistics']['average'] for r in successful_results]\n",
    "        overall_avg = sum(all_averages) / len(all_averages)\n",
    "        overall_min = min(all_averages)\n",
    "        overall_max = max(all_averages)\n",
    "        \n",
    "        f.write(f\"\\n\\nOVERALL STUDY STATISTICS\\n\")\n",
    "        f.write(\"-\" * 25 + \"\\n\")\n",
    "        f.write(f\"Study-wide average inflammation: {overall_avg:.3f}\\n\")\n",
    "        f.write(f\"Patient average range: {overall_min:.3f} - {overall_max:.3f}\\n\")\n",
    "        \n",
    "        # Risk distribution\n",
    "        high_risk = sum(1 for avg in all_averages if avg > 4.0)\n",
    "        moderate_risk = sum(1 for avg in all_averages if 2.5 < avg <= 4.0)\n",
    "        low_risk = sum(1 for avg in all_averages if avg <= 2.5)\n",
    "        \n",
    "        f.write(f\"\\nRisk distribution:\\n\")\n",
    "        f.write(f\"  High risk (>4.0): {high_risk} patients ({high_risk/len(all_averages)*100:.1f}%)\\n\")\n",
    "        f.write(f\"  Moderate risk (2.5-4.0): {moderate_risk} patients ({moderate_risk/len(all_averages)*100:.1f}%)\\n\")\n",
    "        f.write(f\"  Low risk (<2.5): {low_risk} patients ({low_risk/len(all_averages)*100:.1f}%)\\n\")\n",
    "        \n",
    "        # Recommendations\n",
    "        f.write(f\"\\nRECOMMENDATIONS\\n\")\n",
    "        f.write(\"-\" * 15 + \"\\n\")\n",
    "        if high_risk > 0:\n",
    "            f.write(f\"â€¢ {high_risk} patients require immediate follow-up\\n\")\n",
    "        if moderate_risk > 0:\n",
    "            f.write(f\"â€¢ {moderate_risk} patients need continued monitoring\\n\")\n",
    "        if overall_avg > 3.0:\n",
    "            f.write(f\"â€¢ Study-wide inflammation levels are elevated - investigate environmental factors\\n\")\n",
    "        \n",
    "        f.write(f\"\\nEnd of report.\\n\")\n",
    "    \n",
    "    print(f\"Summary report written to: {output_filename}\")\n",
    "    return output_filename\n",
    "\n",
    "# Generate summary report\n",
    "report_file = write_summary_report(analysis_results)\n",
    "\n",
    "# Read and display the report\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"GENERATED REPORT PREVIEW:\")\n",
    "print(\"=\"*50)\n",
    "with open(report_file, 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e84cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write detailed CSV results for further analysis\n",
    "def write_csv_results(results, output_filename='detailed_results.csv'):\n",
    "    \"\"\"Write detailed results in CSV format.\"\"\"\n",
    "    \n",
    "    successful_results = [r for r in results if r['status'] == 'success']\n",
    "    \n",
    "    with open(output_filename, 'w') as f:\n",
    "        # Write header\n",
    "        f.write(\"Patient_ID,Filename,Days_Monitored,Average_Inflammation,\"\n",
    "               \"Min_Inflammation,Max_Inflammation,Std_Dev,Risk_Level\\n\")\n",
    "        \n",
    "        # Write data\n",
    "        for result in successful_results:\n",
    "            stats = result['statistics']\n",
    "            \n",
    "            # Determine risk level\n",
    "            if stats['average'] > 4.0:\n",
    "                risk_level = \"High\"\n",
    "            elif stats['average'] > 2.5:\n",
    "                risk_level = \"Moderate\"\n",
    "            else:\n",
    "                risk_level = \"Low\"\n",
    "            \n",
    "            f.write(f\"{result['patient_id']},\"\n",
    "                   f\"{result['filename']},\"\n",
    "                   f\"{stats['count']},\"\n",
    "                   f\"{stats['average']:.3f},\"\n",
    "                   f\"{stats['minimum']:.3f},\"\n",
    "                   f\"{stats['maximum']:.3f},\"\n",
    "                   f\"{stats['std_dev']:.3f},\"\n",
    "                   f\"{risk_level}\\n\")\n",
    "    \n",
    "    print(f\"Detailed CSV results written to: {output_filename}\")\n",
    "    return output_filename\n",
    "\n",
    "csv_file = write_csv_results(analysis_results)\n",
    "\n",
    "# Preview the CSV file\n",
    "print(\"\\nCSV file preview:\")\n",
    "with open(csv_file, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    for i, line in enumerate(lines):\n",
    "        print(f\"{i+1:2d}: {line.strip()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218d0886",
   "metadata": {},
   "source": [
    "## 7. File Organization and Batch Processing\n",
    "\n",
    "Organize files and create processing workflows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494d613a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create organized directory structure\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "def organize_analysis_files():\n",
    "    \"\"\"Organize analysis files into a structured directory.\"\"\"\n",
    "    \n",
    "    # Create organized structure\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    analysis_dir = f\"analysis_{timestamp}\"\n",
    "    \n",
    "    directories = {\n",
    "        'data': f\"{analysis_dir}/data\",\n",
    "        'results': f\"{analysis_dir}/results\",\n",
    "        'reports': f\"{analysis_dir}/reports\",\n",
    "        'plots': f\"{analysis_dir}/plots\"\n",
    "    }\n",
    "    \n",
    "    # Create directories\n",
    "    for name, path in directories.items():\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        print(f\"Created directory: {path}\")\n",
    "    \n",
    "    # Copy data files\n",
    "    data_files = glob.glob('inflammation_data/*.csv')\n",
    "    for file in data_files:\n",
    "        dest = os.path.join(directories['data'], os.path.basename(file))\n",
    "        shutil.copy2(file, dest)\n",
    "    print(f\"Copied {len(data_files)} data files\")\n",
    "    \n",
    "    # Move result files\n",
    "    result_files = ['analysis_summary.txt', 'detailed_results.csv']\n",
    "    for file in result_files:\n",
    "        if os.path.exists(file):\n",
    "            dest = os.path.join(directories['reports'], file)\n",
    "            shutil.move(file, dest)\n",
    "            print(f\"Moved {file} to reports/\")\n",
    "    \n",
    "    # Create analysis log\n",
    "    log_file = os.path.join(directories['results'], 'analysis_log.txt')\n",
    "    with open(log_file, 'w') as f:\n",
    "        f.write(f\"Inflammation Analysis Log\\n\")\n",
    "        f.write(f\"Analysis started: {datetime.now()}\\n\")\n",
    "        f.write(f\"Files processed: {len(data_files)}\\n\")\n",
    "        f.write(f\"Analysis directory: {analysis_dir}\\n\")\n",
    "        f.write(f\"\\nFile structure:\\n\")\n",
    "        for name, path in directories.items():\n",
    "            f.write(f\"  {name}: {path}\\n\")\n",
    "    \n",
    "    print(f\"Created analysis log: {log_file}\")\n",
    "    return analysis_dir, directories\n",
    "\n",
    "# Organize files\n",
    "analysis_dir, dirs = organize_analysis_files()\n",
    "\n",
    "print(f\"\\nAnalysis organized in: {analysis_dir}\")\n",
    "print(\"Directory structure:\")\n",
    "for name, path in dirs.items():\n",
    "    files = os.listdir(path)\n",
    "    print(f\"  {name}: {len(files)} files\")\n",
    "    for file in files[:3]:  # Show first 3 files\n",
    "        print(f\"    - {file}\")\n",
    "    if len(files) > 3:\n",
    "        print(f\"    ... and {len(files) - 3} more\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db2621d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete batch processing workflow\n",
    "def complete_inflammation_analysis_workflow(data_pattern, output_dir=None):\n",
    "    \"\"\"Complete workflow for inflammation data analysis.\"\"\"\n",
    "    \n",
    "    print(\"ðŸ”¬ INFLAMMATION DATA ANALYSIS WORKFLOW\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Step 1: Find files\n",
    "    print(\"Step 1: Finding data files...\")\n",
    "    files = glob.glob(data_pattern)\n",
    "    print(f\"  Found {len(files)} files matching pattern: {data_pattern}\")\n",
    "    \n",
    "    if not files:\n",
    "        print(\"  âŒ No files found! Check the pattern.\")\n",
    "        return None\n",
    "    \n",
    "    # Step 2: Process files\n",
    "    print(\"\\nStep 2: Processing files...\")\n",
    "    results = process_all_inflammation_files(data_pattern)\n",
    "    successful = [r for r in results if r['status'] == 'success']\n",
    "    print(f\"  âœ… Successfully processed: {len(successful)} files\")\n",
    "    print(f\"  âŒ Failed: {len(results) - len(successful)} files\")\n",
    "    \n",
    "    if not successful:\n",
    "        print(\"  âŒ No successful analyses!\")\n",
    "        return None\n",
    "    \n",
    "    # Step 3: Generate reports\n",
    "    print(\"\\nStep 3: Generating reports...\")\n",
    "    summary_file = write_summary_report(results, 'workflow_summary.txt')\n",
    "    csv_file = write_csv_results(results, 'workflow_results.csv')\n",
    "    print(f\"  ðŸ“„ Summary report: {summary_file}\")\n",
    "    print(f\"  ðŸ“Š CSV results: {csv_file}\")\n",
    "    \n",
    "    # Step 4: Calculate key insights\n",
    "    print(\"\\nStep 4: Key insights...\")\n",
    "    averages = [r['statistics']['average'] for r in successful]\n",
    "    high_risk_count = sum(1 for avg in averages if avg > 4.0)\n",
    "    study_avg = sum(averages) / len(averages)\n",
    "    \n",
    "    print(f\"  ðŸ“ˆ Study average inflammation: {study_avg:.2f}\")\n",
    "    print(f\"  âš ï¸  High-risk patients: {high_risk_count}/{len(successful)} ({high_risk_count/len(successful)*100:.1f}%)\")\n",
    "    print(f\"  ðŸ“Š Patient range: {min(averages):.2f} - {max(averages):.2f}\")\n",
    "    \n",
    "    # Step 5: Organize results\n",
    "    if output_dir:\n",
    "        print(f\"\\nStep 5: Organizing results in {output_dir}...\")\n",
    "        # Move files to organized structure\n",
    "        # Implementation would go here\n",
    "    \n",
    "    print(\"\\nðŸŽ‰ Workflow completed successfully!\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    return {\n",
    "        'results': results,\n",
    "        'successful_count': len(successful),\n",
    "        'study_average': study_avg,\n",
    "        'high_risk_count': high_risk_count,\n",
    "        'summary_file': summary_file,\n",
    "        'csv_file': csv_file\n",
    "    }\n",
    "\n",
    "# Run complete workflow\n",
    "workflow_results = complete_inflammation_analysis_workflow(\n",
    "    data_pattern=f\"{analysis_dir}/data/*.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26d025f",
   "metadata": {},
   "source": [
    "### Exercise 6.2\n",
    "Create a monitoring system that:\n",
    "1. Watches for new data files\n",
    "2. Automatically processes them\n",
    "3. Updates summary statistics\n",
    "4. Alerts on high-risk patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd85a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 6.2 - Your monitoring system\n",
    "class InflammationMonitor:\n",
    "    \"\"\"Monitor for new inflammation data files and process them automatically.\"\"\"\n",
    "    \n",
    "    def __init__(self, watch_directory, alert_threshold=4.0):\n",
    "        # Your initialization here\n",
    "        pass\n",
    "    \n",
    "    def scan_for_new_files(self):\n",
    "        \"\"\"Scan for files that haven't been processed yet.\"\"\"\n",
    "        # Your implementation here\n",
    "        pass\n",
    "    \n",
    "    def process_new_file(self, filename):\n",
    "        \"\"\"Process a newly detected file.\"\"\"\n",
    "        # Your implementation here\n",
    "        pass\n",
    "    \n",
    "    def check_for_alerts(self, result):\n",
    "        \"\"\"Check if patient needs immediate attention.\"\"\"\n",
    "        # Your implementation here\n",
    "        pass\n",
    "    \n",
    "    def update_summary(self):\n",
    "        \"\"\"Update overall summary statistics.\"\"\"\n",
    "        # Your implementation here\n",
    "        pass\n",
    "    \n",
    "    def run_monitoring_cycle(self):\n",
    "        \"\"\"Run one cycle of monitoring.\"\"\"\n",
    "        # Your implementation here\n",
    "        pass\n",
    "\n",
    "# Test your monitoring system\n",
    "# Add test code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78878cc",
   "metadata": {},
   "source": [
    "## 8. Working with Different File Formats\n",
    "\n",
    "Handle various data formats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d9feac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample files in different formats\n",
    "\n",
    "# 1. Tab-separated values\n",
    "with open('sample_data.tsv', 'w') as f:\n",
    "    f.write(\"Day\\tInflammation\\tTemperature\\n\")\n",
    "    f.write(\"1\\t1.5\\t36.8\\n\")\n",
    "    f.write(\"2\\t2.3\\t37.2\\n\")\n",
    "    f.write(\"3\\t1.8\\t36.9\\n\")\n",
    "\n",
    "# 2. JSON format\n",
    "import json\n",
    "json_data = {\n",
    "    \"patient_id\": \"P999\",\n",
    "    \"study_date\": \"2024-01-15\",\n",
    "    \"measurements\": [\n",
    "        {\"day\": 1, \"inflammation\": 1.5, \"temperature\": 36.8},\n",
    "        {\"day\": 2, \"inflammation\": 2.3, \"temperature\": 37.2},\n",
    "        {\"day\": 3, \"inflammation\": 1.8, \"temperature\": 36.9}\n",
    "    ]\n",
    "}\n",
    "\n",
    "with open('sample_data.json', 'w') as f:\n",
    "    json.dump(json_data, f, indent=2)\n",
    "\n",
    "# 3. Fixed-width format\n",
    "with open('sample_data.txt', 'w') as f:\n",
    "    f.write(\"Day Inflammation Temperature\\n\")\n",
    "    f.write(\"  1        1.5       36.8\\n\")\n",
    "    f.write(\"  2        2.3       37.2\\n\")\n",
    "    f.write(\"  3        1.8       36.9\\n\")\n",
    "\n",
    "print(\"Created sample files in different formats:\")\n",
    "for filename in ['sample_data.tsv', 'sample_data.json', 'sample_data.txt']:\n",
    "    print(f\"  {filename} ({os.path.getsize(filename)} bytes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d9d55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Universal file parser\n",
    "def parse_any_format(filename):\n",
    "    \"\"\"Parse inflammation data from various file formats.\"\"\"\n",
    "    \n",
    "    _, ext = os.path.splitext(filename.lower())\n",
    "    \n",
    "    try:\n",
    "        if ext == '.json':\n",
    "            return parse_json_format(filename)\n",
    "        elif ext == '.tsv':\n",
    "            return parse_tsv_format(filename)\n",
    "        elif ext in ['.txt', '.dat']:\n",
    "            return parse_text_format(filename)\n",
    "        elif ext == '.csv':\n",
    "            return parse_csv_inflammation_file(filename)\n",
    "        else:\n",
    "            # Try to auto-detect format\n",
    "            return auto_detect_and_parse(filename)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing {filename}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def parse_json_format(filename):\n",
    "    \"\"\"Parse JSON format inflammation data.\"\"\"\n",
    "    with open(filename, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    measurements = data.get('measurements', [])\n",
    "    inflammation_data = [m['inflammation'] for m in measurements]\n",
    "    \n",
    "    metadata = {\n",
    "        'patient_id': data.get('patient_id', 'Unknown'),\n",
    "        'study_date': data.get('study_date', 'Unknown'),\n",
    "        'filename': filename\n",
    "    }\n",
    "    \n",
    "    return inflammation_data, metadata\n",
    "\n",
    "def parse_tsv_format(filename):\n",
    "    \"\"\"Parse tab-separated values format.\"\"\"\n",
    "    inflammation_data = []\n",
    "    metadata = {'filename': filename}\n",
    "    \n",
    "    with open(filename, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        \n",
    "        # Skip header if present\n",
    "        start_line = 1 if 'Day' in lines[0] else 0\n",
    "        \n",
    "        for line in lines[start_line:]:\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) >= 2:\n",
    "                try:\n",
    "                    inflammation = float(parts[1])\n",
    "                    inflammation_data.append(inflammation)\n",
    "                except ValueError:\n",
    "                    continue\n",
    "    \n",
    "    return inflammation_data, metadata\n",
    "\n",
    "def parse_text_format(filename):\n",
    "    \"\"\"Parse fixed-width text format.\"\"\"\n",
    "    inflammation_data = []\n",
    "    metadata = {'filename': filename}\n",
    "    \n",
    "    with open(filename, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        \n",
    "        # Skip header if present\n",
    "        start_line = 1 if any(word in lines[0].lower() for word in ['day', 'inflammation']) else 0\n",
    "        \n",
    "        for line in lines[start_line:]:\n",
    "            # Split by whitespace and try to find inflammation value\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) >= 2:\n",
    "                try:\n",
    "                    inflammation = float(parts[1])  # Assume inflammation is second column\n",
    "                    inflammation_data.append(inflammation)\n",
    "                except ValueError:\n",
    "                    continue\n",
    "    \n",
    "    return inflammation_data, metadata\n",
    "\n",
    "def auto_detect_and_parse(filename):\n",
    "    \"\"\"Auto-detect format and parse accordingly.\"\"\"\n",
    "    with open(filename, 'r') as f:\n",
    "        first_line = f.readline().strip()\n",
    "    \n",
    "    if first_line.startswith('{'):\n",
    "        return parse_json_format(filename)\n",
    "    elif '\\t' in first_line:\n",
    "        return parse_tsv_format(filename)\n",
    "    elif ',' in first_line:\n",
    "        return parse_csv_inflammation_file(filename)\n",
    "    else:\n",
    "        return parse_text_format(filename)\n",
    "\n",
    "# Test universal parser\n",
    "test_files = ['sample_data.json', 'sample_data.tsv', 'sample_data.txt']\n",
    "\n",
    "print(\"Testing universal parser:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "for filename in test_files:\n",
    "    data, metadata = parse_any_format(filename)\n",
    "    if data:\n",
    "        avg = sum(data) / len(data) if data else 0\n",
    "        print(f\"âœ… {filename:<18} | {len(data):2d} readings | avg: {avg:.2f}\")\n",
    "        print(f\"   Data: {data}\")\n",
    "    else:\n",
    "        print(f\"âŒ {filename:<18} | Failed to parse\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14418ca",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this episode, we learned:\n",
    "- **File operations**: Reading and writing files in Python\n",
    "- **File parsing**: Extracting data from structured files\n",
    "- **Multiple files**: Processing batches of files automatically\n",
    "- **Glob patterns**: Finding files using wildcards\n",
    "- **Data organization**: Creating structured analysis workflows\n",
    "- **Different formats**: Handling CSV, JSON, TSV, and text files\n",
    "- **Error handling**: Robust file processing with error recovery\n",
    "- **Automation**: Building complete analysis pipelines\n",
    "\n",
    "Working with files is essential for real-world data analysis projects!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
